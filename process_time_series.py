import pandas as pd
import numpy as np
from scipy import stats
from scipy.signal import find_peaks
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç –¥–ª—è LSTM
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Patch

def calculate_time_series_features(segment, segment_id, segment_start_idx, target_columns):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞.
    –¢–µ–ø–µ—Ä—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.
    """
    if len(segment) < 5:
        return None
    
    features = {
        '–°–µ–≥–º–µ–Ω—Ç_ID': segment_id,
        '–ù–∞—á–∞–ª—å–Ω—ã–π_–∏–Ω–¥–µ–∫—Å': segment_start_idx,
        '–î–ª–∏–Ω–∞_—Å–µ–≥–º–µ–Ω—Ç–∞': len(segment),
    }
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤—ã—á–∏—Å–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    for target_col in target_columns:
        time_series = segment[target_col].values
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
        if np.isnan(time_series).any():
            # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            time_series = np.nan_to_num(time_series, nan=np.nanmean(time_series))
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        features[f'{target_col}_–ú–∏–Ω–∏–º—É–º'] = np.min(time_series)
        features[f'{target_col}_–ú–∞–∫—Å–∏–º—É–º'] = np.max(time_series)
        features[f'{target_col}_–°—Ä–µ–¥–Ω–µ–µ'] = np.mean(time_series)
        features[f'{target_col}_–ú–µ–¥–∏–∞–Ω–∞'] = np.median(time_series)
        features[f'{target_col}_–î–∏—Å–ø–µ—Ä—Å–∏—è'] = np.var(time_series)
        features[f'{target_col}_–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ_–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ'] = np.std(time_series)
        features[f'{target_col}_–†–∞–∑–º–∞—Ö'] = np.max(time_series) - np.min(time_series)
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ –∏ —ç–∫—Å—Ü–µ—Å—Å–∞
        if len(time_series) > 2 and np.std(time_series) > 0:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_–∞—Å–∏–º–º–µ—Ç—Ä–∏–∏'] = stats.skew(time_series)
        else:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_–∞—Å–∏–º–º–µ—Ç—Ä–∏–∏'] = 0
            
        if len(time_series) > 3 and np.std(time_series) > 0:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—ç–∫—Å—Ü–µ—Å—Å–∞'] = stats.kurtosis(time_series)
        else:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—ç–∫—Å—Ü–µ—Å—Å–∞'] = 0
        
        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è (–ª–∞–≥ 1)
        if len(time_series) > 1 and np.std(time_series) > 0:
            autocorr = pd.Series(time_series).autocorr(lag=1)
            features[f'{target_col}_–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è_–ª–∞–≥1'] = autocorr if not np.isnan(autocorr) else 0
        else:
            features[f'{target_col}_–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è_–ª–∞–≥1'] = 0
        
        # –ü–ª–æ—â–∞–¥—å –ø–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º
        features[f'{target_col}_–ü–ª–æ—â–∞–¥—å_–ø–æ–¥_–≥—Ä–∞—Ñ–∏–∫–æ–º'] = np.trapz(time_series)
        
        # –ù–∞–∫–ª–æ–Ω —Ç—Ä–µ–Ω–¥–∞
        x = np.arange(len(time_series))
        if len(time_series) > 1 and np.var(time_series) > 0:
            try:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, time_series)
                features[f'{target_col}_–ù–∞–∫–ª–æ–Ω_—Ç—Ä–µ–Ω–¥–∞'] = slope
                features[f'{target_col}_R_–∫–≤–∞–¥—Ä–∞—Ç_—Ç—Ä–µ–Ω–¥–∞'] = r_value**2
            except:
                features[f'{target_col}_–ù–∞–∫–ª–æ–Ω_—Ç—Ä–µ–Ω–¥–∞'] = 0
                features[f'{target_col}_R_–∫–≤–∞–¥—Ä–∞—Ç_—Ç—Ä–µ–Ω–¥–∞'] = 0
        else:
            features[f'{target_col}_–ù–∞–∫–ª–æ–Ω_—Ç—Ä–µ–Ω–¥–∞'] = 0
            features[f'{target_col}_R_–∫–≤–∞–¥—Ä–∞—Ç_—Ç—Ä–µ–Ω–¥–∞'] = 0
        
        # –ö–≤–∞—Ä—Ç–∏–ª–∏ –∏ IQR
        if len(time_series) >= 4:
            q1 = np.percentile(time_series, 25)
            q3 = np.percentile(time_series, 75)
            features[f'{target_col}_Q1'] = q1
            features[f'{target_col}_Q3'] = q3
            features[f'{target_col}_IQR'] = q3 - q1
        else:
            features[f'{target_col}_Q1'] = 0
            features[f'{target_col}_Q3'] = 0
            features[f'{target_col}_IQR'] = 0
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫–æ–≤
        try:
            peaks, _ = find_peaks(time_series)
            features[f'{target_col}_–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø–∏–∫–æ–≤'] = len(peaks)
        except:
            features[f'{target_col}_–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø–∏–∫–æ–≤'] = 0
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
        if np.mean(time_series) != 0 and np.std(time_series) > 0:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_–≤–∞—Ä–∏–∞—Ü–∏–∏'] = np.std(time_series) / np.mean(time_series)
        else:
            features[f'{target_col}_–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_–≤–∞—Ä–∏–∞—Ü–∏–∏'] = 0
        
        # –°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        features[f'{target_col}_–°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–µ'] = np.sqrt(np.mean(time_series**2))
    
    return features

def perform_kmeans_clustering(features_df, n_clusters=3):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é KMeans —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN –∑–Ω–∞—á–µ–Ω–∏–π.
    """
    print("\n" + "=" * 60)
    print("–ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø K-MEANS")
    print("=" * 60)
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    numeric_cols = features_df.select_dtypes(include=[np.number]).columns
    # –ò—Å–∫–ª—é—á–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏ –∏–Ω–¥–µ–∫—Å—ã
    exclude_cols = ['–ù–∞—á–∞–ª—å–Ω—ã–π_–∏–Ω–¥–µ–∫—Å', '–î–ª–∏–Ω–∞_—Å–µ–≥–º–µ–Ω—Ç–∞']
    cluster_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    X = features_df[cluster_cols].values
    
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {X.shape}")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {len(cluster_cols)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
    nan_count = np.isnan(X).sum()
    if nan_count > 0:
        print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
        print("–ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏...")
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
        imputer = SimpleImputer(strategy='mean')
        X = imputer.fit_transform(X)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (StandardScaler)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –ª–æ–∫—Ç—è
    if n_clusters == 'auto':
        print("\n–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è)...")
        inertias = []
        max_clusters = min(10, len(X_scaled))
        
        for k in range(1, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X_scaled)
            inertias.append(kmeans.inertia_)
        
        # –ù–∞—Ö–æ–¥–∏–º "–ª–æ–∫–æ—Ç—å" - —Ç–æ—á–∫—É, –≥–¥–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –∏–Ω–µ—Ä—Ü–∏–∏ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è
        diffs = np.diff(inertias)
        diff_diffs = np.diff(diffs)
        if len(diff_diffs) > 0:
            n_clusters = np.argmax(diff_diffs) + 2
        else:
            n_clusters = 3
        
        print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ KMeans
    print(f"\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ KMeans —Å {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ DataFrame
    features_df['–ö–ª–∞—Å—Ç–µ—Ä'] = cluster_labels
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ:")
    
    for cluster_id in sorted(set(cluster_labels)):
        count = list(cluster_labels).count(cluster_id)
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ({count/len(cluster_labels)*100:.1f}%)")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–µ—Ä—Ü–∏—é (—Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤)
    inertia = kmeans.inertia_
    print(f"\n–ò–Ω–µ—Ä—Ü–∏—è (within-cluster sum of squares): {inertia:.2f}")
    
    return features_df, cluster_labels, scaler, cluster_cols, kmeans

def prepare_cross_segment_data(train_segment, test_segment, input_columns, output_columns, sequence_length=10):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –¥—Ä—É–≥–æ–º.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
    train_data = train_segment.fillna(train_segment.mean())
    test_data = test_segment.fillna(test_segment.mean())
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    X_train_data = train_data[input_columns].values
    y_train_data = train_data[output_columns].values
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    X_test_data = test_data[input_columns].values
    y_test_data = test_data[output_columns].values
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏)
    X_scaler = MinMaxScaler(feature_range=(0, 1))
    y_scaler = MinMaxScaler(feature_range=(0, 1))
    
    X_train_scaled = X_scaler.fit_transform(X_train_data)
    y_train_scaled = y_scaler.fit_transform(y_train_data)
    
    # –î–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Å–∫–µ–π–ª–µ—Ä—ã
    X_test_scaled = X_scaler.transform(X_test_data)
    y_test_scaled = y_scaler.transform(y_test_data)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    X_train_seq, y_train_seq = [], []
    for i in range(len(X_train_scaled) - sequence_length):
        X_train_seq.append(X_train_scaled[i:i+sequence_length])
        y_train_seq.append(y_train_scaled[i+sequence_length])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    X_test_seq, y_test_seq = [], []
    for i in range(len(X_test_scaled) - sequence_length):
        X_test_seq.append(X_test_scaled[i:i+sequence_length])
        y_test_seq.append(y_test_scaled[i+sequence_length])
    
    if len(X_train_seq) == 0 or len(X_test_seq) == 0:
        return None, None, None, None, None, None
    
    X_train_seq = np.array(X_train_seq)
    y_train_seq = np.array(y_train_seq)
    X_test_seq = np.array(X_test_seq)
    y_test_seq = np.array(y_test_seq)
    
    return X_train_seq, X_test_seq, y_train_seq, y_test_seq, X_scaler, y_scaler

def build_improved_lstm_model(input_shape, output_dim, units=64, dropout_rate=0.3, l2_reg=0.001):
    """
    –°—Ç—Ä–æ–∏—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å LSTM –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.
    """
    model = Sequential([
        Input(shape=input_shape),
        LSTM(units, return_sequences=True, kernel_regularizer=l2(l2_reg), 
             recurrent_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        LSTM(units//2, return_sequences=True, kernel_regularizer=l2(l2_reg),
             recurrent_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        LSTM(units//4, return_sequences=False, kernel_regularizer=l2(l2_reg),
             recurrent_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        Dropout(dropout_rate/2),
        
        Dense(32, activation='relu', kernel_regularizer=l2(l2_reg)),
        BatchNormalization(),
        
        Dense(output_dim)
    ])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate
    optimizer = Adam(learning_rate=0.001, clipvalue=1.0)
    
    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    return model

def train_and_test_cross_segment(cluster_data, cluster_id, input_columns, output_columns, 
                                sequence_length=10, epochs=100):
    """
    –û–±—É—á–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–∞ –¥—Ä—É–≥–æ–º –≤ —Ç–æ–º –∂–µ –∫–ª–∞—Å—Ç–µ—Ä–µ.
    """
    print(f"\n{'='*60}")
    print(f"–ö–õ–ê–°–¢–ï–† {cluster_id}: –û–ë–£–ß–ï–ù–ò–ï –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print(f"{'='*60}")
    
    segment_ids = list(cluster_data.keys())
    
    if len(segment_ids) < 2:
        print(f"‚ùå –í –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 2)")
        return None, None, None, None
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    train_seg_id = segment_ids[0]
    test_seg_id = segment_ids[1]
    
    print(f"üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø:")
    print(f"   –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {train_seg_id}")
    print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {test_seg_id}")
    
    train_segment = cluster_data[train_seg_id]
    test_segment = cluster_data[test_seg_id]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    prepared_data = prepare_cross_segment_data(
        train_segment, test_segment, input_columns, output_columns, sequence_length
    )
    
    if prepared_data[0] is None:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return None, None, None, None
    
    X_train, X_test, y_train, y_test, X_scaler, y_scaler = prepared_data
    
    if len(X_train) < 1:
        print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return None, None, None, None
    
    if len(X_test) < 1:
        print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return None, None, None, None
    
    print(f"üìä –î–ê–ù–ù–´–ï:")
    print(f"   –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"   –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_test)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    model = build_improved_lstm_model(
        input_shape=(sequence_length, len(input_columns)),
        output_dim=len(output_columns)
    )
    
    # Callbacks –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        min_delta=0.0001,
        verbose=0
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=0.00001,
        verbose=0
    )
    
    print("üèãÔ∏è –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò...")
    history = model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=epochs,
        batch_size=32,
        callbacks=[early_stopping, reduce_lr],
        verbose=0
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    if len(history.history['loss']) < epochs:
        print(f"   –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ —ç–ø–æ—Ö–µ {len(history.history['loss'])} (—Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞)")
    
    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("üîÆ –í–´–ü–û–õ–ù–Ø–ï–ú –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–ï...")
    y_pred_scaled = model.predict(X_test, verbose=0)
    y_pred = y_scaler.inverse_transform(y_pred_scaled)
    y_test_original = y_scaler.inverse_transform(y_test)
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
    metrics = {
        '–ö–ª–∞—Å—Ç–µ—Ä': cluster_id,
        '–°–µ–≥–º–µ–Ω—Ç_–æ–±—É—á–µ–Ω–∏—è': train_seg_id,
        '–°–µ–≥–º–µ–Ω—Ç_—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è': test_seg_id,
        '–î–ª–∏–Ω–∞_–æ–±—É—á–∞—é—â–µ–π_–≤—ã–±–æ—Ä–∫–∏': len(X_train),
        '–î–ª–∏–Ω–∞_—Ç–µ—Å—Ç–æ–≤–æ–π_–≤—ã–±–æ—Ä–∫–∏': len(X_test),
        '–≠–ø–æ—Ö–∏_–æ–±—É—á–µ–Ω–∏—è': len(history.history['loss'])
    }
    
    param_metrics = {}
    for i, output_col in enumerate(output_columns):
        mse = mean_squared_error(y_test_original[:, i], y_pred[:, i])
        mae = mean_absolute_error(y_test_original[:, i], y_pred[:, i])
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test_original[:, i], y_pred[:, i])
        
        param_metrics[f'{output_col}_MSE'] = mse
        param_metrics[f'{output_col}_RMSE'] = rmse
        param_metrics[f'{output_col}_MAE'] = mae
        param_metrics[f'{output_col}_R2'] = r2
    
    # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
    metrics['–°—Ä–µ–¥–Ω–∏–π_MSE'] = np.mean([param_metrics[f'{col}_MSE'] for col in output_columns])
    metrics['–°—Ä–µ–¥–Ω–∏–π_RMSE'] = np.mean([param_metrics[f'{col}_RMSE'] for col in output_columns])
    metrics['–°—Ä–µ–¥–Ω–∏–π_MAE'] = np.mean([param_metrics[f'{col}_MAE'] for col in output_columns])
    metrics['–°—Ä–µ–¥–Ω–∏–π_R2'] = np.mean([param_metrics[f'{col}_R2'] for col in output_columns])
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
    all_metrics = {**metrics, **param_metrics}
    
    print(f"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û:")
    print(f"   –°—Ä–µ–¥–Ω–∏–π R¬≤ = {metrics['–°—Ä–µ–¥–Ω–∏–π_R2']:.4f}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π MSE = {metrics['–°—Ä–µ–¥–Ω–∏–π_MSE']:.8f}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π MAE = {metrics['–°—Ä–µ–¥–Ω–∏–π_MAE']:.8f}")
    
    return {
        'y_test': y_test_original,
        'y_pred': y_pred,
        'history': history.history,
        'model': model,
        'train_segment': train_seg_id,
        'test_segment': test_seg_id,
        'X_scaler': X_scaler,
        'y_scaler': y_scaler
    }, all_metrics, train_seg_id, test_seg_id

def create_detailed_prediction_plots(cluster_id, predictions, metrics, output_columns, train_seg_id, test_seg_id):
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
    """
    import os
    cluster_dir = f"cluster_{cluster_id}_results"
    os.makedirs(cluster_dir, exist_ok=True)
    
    y_test = predictions['y_test']
    y_pred = predictions['y_pred']
    
    # 1. –ì—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
    for i, output_col in enumerate(output_columns):
        plt.figure(figsize=(14, 8))
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        plt.subplot(2, 2, 1)
        time_steps = range(len(y_test[:, i]))
        
        plt.plot(time_steps, y_test[:, i], 'b-', linewidth=2, alpha=0.7, label='–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.plot(time_steps, y_pred[:, i], 'r--', linewidth=2, alpha=0.7, label='–ü—Ä–æ–≥–Ω–æ–∑—ã')
        plt.fill_between(time_steps, y_test[:, i], y_pred[:, i], alpha=0.2, color='gray', label='–û—à–∏–±–∫–∞')
        
        plt.title(f'{output_col}\n–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}', fontsize=14, fontweight='bold')
        plt.xlabel('–í—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥', fontsize=12)
        plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–æ–∫
        plt.subplot(2, 2, 2)
        errors = y_test[:, i] - y_pred[:, i]
        
        plt.plot(time_steps, errors, 'g-', linewidth=1.5, alpha=0.7)
        plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)
        plt.fill_between(time_steps, errors, 0, alpha=0.2, color='green')
        
        plt.title(f'–û—à–∏–±–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\nMSE={metrics[f"{output_col}_MSE"]:.6f}, MAE={metrics[f"{output_col}_MAE"]:.6f}', 
                 fontsize=12)
        plt.xlabel('–í—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥', fontsize=10)
        plt.ylabel('–û—à–∏–±–∫–∞', fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—à–∏–±–æ–∫
        plt.subplot(2, 2, 3)
        plt.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
        plt.axvline(x=0, color='r', linestyle='--', linewidth=2)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫', fontsize=12)
        plt.xlabel('–û—à–∏–±–∫–∞', fontsize=10)
        plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=10)
        plt.grid(True, alpha=0.3, axis='y')
        
        # –î–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å—Å–µ—è–Ω–∏—è
        plt.subplot(2, 2, 4)
        plt.scatter(y_test[:, i], y_pred[:, i], alpha=0.6, s=30)
        
        # –õ–∏–Ω–∏—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
        min_val = min(y_test[:, i].min(), y_pred[:, i].min())
        max_val = max(y_test[:, i].max(), y_pred[:, i].max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, alpha=0.7)
        
        plt.title(f'–î–∏–∞–≥—Ä–∞–º–º–∞ —Ä–∞—Å—Å–µ—è–Ω–∏—è\nR¬≤={metrics[f"{output_col}_R2"]:.4f}', fontsize=12)
        plt.xlabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=10)
        plt.ylabel('–ü—Ä–æ–≥–Ω–æ–∑—ã', fontsize=10)
        plt.grid(True, alpha=0.3)
        
        plt.suptitle(f'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {output_col}\n'
                    f'–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {train_seg_id}, –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {test_seg_id}', 
                    fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(f"{cluster_dir}/detailed_prediction_{output_col}_cluster_{cluster_id}.png", 
                   dpi=150, bbox_inches='tight')
        plt.close()
    
    # 2. –°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    plt.figure(figsize=(16, 10))
    
    n_params = len(output_columns)
    n_cols = 3
    n_rows = (n_params + n_cols - 1) // n_cols
    
    for i, output_col in enumerate(output_columns):
        plt.subplot(n_rows, n_cols, i + 1)
        
        time_steps = range(len(y_test[:, i]))
        
        plt.plot(time_steps, y_test[:, i], 'b-', linewidth=1.5, alpha=0.7, label='–ò—Å—Ç–∏–Ω–Ω—ã–µ')
        plt.plot(time_steps, y_pred[:, i], 'r--', linewidth=1.5, alpha=0.7, label='–ü—Ä–æ–≥–Ω–æ–∑—ã')
        
        plt.title(f'{output_col}\nR¬≤={metrics[f"{output_col}_R2"]:.4f}', fontsize=11)
        plt.xlabel('–í—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥', fontsize=9)
        plt.ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ', fontsize=9)
        plt.legend(fontsize=8)
        plt.grid(True, alpha=0.3)
    
    plt.suptitle(f'–°–í–û–î–ö–ê –ü–†–û–ì–ù–û–ó–û–í –ü–û –í–°–ï–ú –ü–ê–†–ê–ú–ï–¢–†–ê–ú - –ö–õ–ê–°–¢–ï–† {cluster_id}\n'
                f'–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {train_seg_id} | –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–µ: {test_seg_id}', 
                fontsize=18, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig(f"{cluster_dir}/summary_predictions_cluster_{cluster_id}.png", 
               dpi=150, bbox_inches='tight')
    plt.close()
    
    # 3. –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 1, 1)
    plt.plot(predictions['history']['loss'], 'b-', label='–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è', linewidth=2)
    plt.plot(predictions['history']['val_loss'], 'r-', label='–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏', linewidth=2)
    plt.title('–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è', fontsize=14, fontweight='bold')
    plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    plt.ylabel('–û—à–∏–±–∫–∞ (MSE)', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.subplot(2, 1, 2)
    plt.plot(predictions['history']['mae'], 'g-', label='MAE –æ–±—É—á–µ–Ω–∏—è', linewidth=2)
    plt.plot([m for m in predictions['history']['val_mae'] if not np.isnan(m)], 'orange', 
             label='MAE –≤–∞–ª–∏–¥–∞—Ü–∏–∏', linewidth=2)
    plt.title('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞', fontsize=14, fontweight='bold')
    plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    plt.ylabel('MAE', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.suptitle(f'–ü–†–û–¶–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø - –ö–õ–ê–°–¢–ï–† {cluster_id}\n'
                f'–°—Ä–µ–¥–Ω–∏–π R¬≤ = {metrics["–°—Ä–µ–¥–Ω–∏–π_R2"]:.4f}', 
                fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig(f"{cluster_dir}/training_history_cluster_{cluster_id}.png", 
               dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"‚úì –°–æ–∑–¥–∞–Ω—ã –¥–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}")

def save_cross_segment_results(cluster_id, predictions, metrics, features_df, output_columns, 
                              train_seg_id, test_seg_id):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    import os
    cluster_dir = f"cluster_{cluster_id}_results"
    os.makedirs(cluster_dir, exist_ok=True)
    
    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ Excel
    metrics_df = pd.DataFrame([metrics])
    metrics_df.to_excel(f"{cluster_dir}/cross_segment_metrics_cluster_{cluster_id}.xlsx", index=False)
    
    # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ Excel
    if predictions:
        with pd.ExcelWriter(f"{cluster_dir}/cross_segment_predictions_cluster_{cluster_id}.xlsx") as writer:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ –¥–ª—è –≤—Å–µ—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            pred_dfs = []
            for i, col in enumerate(output_columns):
                temp_df = pd.DataFrame({
                    f'{col}_–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ': predictions['y_test'][:, i],
                    f'{col}_–ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ': predictions['y_pred'][:, i],
                    f'{col}_–û—à–∏–±–∫–∞': predictions['y_test'][:, i] - predictions['y_pred'][:, i],
                    f'{col}_–ê–±—Å–æ–ª—é—Ç–Ω–∞—è_–æ—à–∏–±–∫–∞': np.abs(predictions['y_test'][:, i] - predictions['y_pred'][:, i]),
                    f'{col}_–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è_–æ—à–∏–±–∫–∞_%': 100 * np.abs(predictions['y_test'][:, i] - predictions['y_pred'][:, i]) / 
                                                  np.abs(predictions['y_test'][:, i] + 1e-10)
                })
                pred_dfs.append(temp_df)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –≤ –æ–¥–∏–Ω DataFrame
            pred_df = pd.concat(pred_dfs, axis=1)
            pred_df.to_excel(writer, sheet_name="predictions", index=False)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
            seg_info = pd.DataFrame({
                '–ü–∞—Ä–∞–º–µ—Ç—Ä': ['–ö–ª–∞—Å—Ç–µ—Ä', '–°–µ–≥–º–µ–Ω—Ç –æ–±—É—á–µ–Ω–∏—è', '–°–µ–≥–º–µ–Ω—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è', 
                           '–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏', '–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏',
                           '–≠–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è', '–°—Ä–µ–¥–Ω–∏–π R¬≤', '–°—Ä–µ–¥–Ω–∏–π MSE'],
                '–ó–Ω–∞—á–µ–Ω–∏–µ': [cluster_id, train_seg_id, test_seg_id,
                           metrics['–î–ª–∏–Ω–∞_–æ–±—É—á–∞—é—â–µ–π_–≤—ã–±–æ—Ä–∫–∏'], metrics['–î–ª–∏–Ω–∞_—Ç–µ—Å—Ç–æ–≤–æ–π_–≤—ã–±–æ—Ä–∫–∏'],
                           metrics['–≠–ø–æ—Ö–∏_–æ–±—É—á–µ–Ω–∏—è'], metrics['–°—Ä–µ–¥–Ω–∏–π_R2'], metrics['–°—Ä–µ–¥–Ω–∏–π_MSE']]
            })
            seg_info.to_excel(writer, sheet_name="segment_info", index=False)
    
    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    cluster_segments = features_df[features_df['–ö–ª–∞—Å—Ç–µ—Ä'] == cluster_id]
    cluster_segments.to_excel(f"{cluster_dir}/segments_info_cluster_{cluster_id}.xlsx", index=False)
    
    # 4. –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
    create_detailed_prediction_plots(cluster_id, predictions, metrics, output_columns, 
                                   train_seg_id, test_seg_id)
    
    print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫—Ä–æ—Å—Å-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    return metrics_df

def create_comprehensive_analysis(all_cluster_results, output_columns, features_df):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
    """
    print("\n" + "=" * 60)
    print("–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 60)
    
    # 1. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    summary_data = []
    
    for cluster_id, results in all_cluster_results.items():
        if not results:
            continue
            
        summary = {
            '–ö–ª–∞—Å—Ç–µ—Ä': cluster_id,
            '–°–µ–≥–º–µ–Ω—Ç_–æ–±—É—á–µ–Ω–∏—è': results['metrics']['–°–µ–≥–º–µ–Ω—Ç_–æ–±—É—á–µ–Ω–∏—è'],
            '–°–µ–≥–º–µ–Ω—Ç_—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è': results['metrics']['–°–µ–≥–º–µ–Ω—Ç_—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è'],
            '–≠–ø–æ—Ö–∏_–æ–±—É—á–µ–Ω–∏—è': results['metrics']['–≠–ø–æ—Ö–∏_–æ–±—É—á–µ–Ω–∏—è'],
            '–°—Ä–µ–¥–Ω–∏–π_R2': results['metrics']['–°—Ä–µ–¥–Ω–∏–π_R2'],
            '–°—Ä–µ–¥–Ω–∏–π_MSE': results['metrics']['–°—Ä–µ–¥–Ω–∏–π_MSE'],
            '–°—Ä–µ–¥–Ω–∏–π_RMSE': results['metrics']['–°—Ä–µ–¥–Ω–∏–π_RMSE'],
            '–°—Ä–µ–¥–Ω–∏–π_MAE': results['metrics']['–°—Ä–µ–¥–Ω–∏–π_MAE']
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        for col in output_columns:
            summary[f'{col}_R2'] = results['metrics'][f'{col}_R2']
            summary[f'{col}_MSE'] = results['metrics'][f'{col}_MSE']
        
        summary_data.append(summary)
    
    if not summary_data:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞")
        return None
    
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    summary_df = pd.DataFrame(summary_data)
    summary_df = summary_df.sort_values('–°—Ä–µ–¥–Ω–∏–π_R2', ascending=False)
    summary_df.to_excel("comprehensive_analysis_summary.xlsx", index=False)
    
    # 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    create_comprehensive_visualizations(all_cluster_results, summary_df, features_df, output_columns)
    
    return summary_df

def create_comprehensive_visualizations(all_cluster_results, summary_df, features_df, output_columns):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    try:
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–µ–¥–Ω–∏–π R2 –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax = axes[0, 0]
        clusters = summary_df['–ö–ª–∞—Å—Ç–µ—Ä'].astype(str)
        r2_values = summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].values
        
        bars = ax.bar(clusters, r2_values, color=plt.cm.viridis(np.linspace(0, 1, len(clusters))))
        ax.set_title('–°—Ä–µ–¥–Ω–∏–π R¬≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º', fontsize=14, fontweight='bold')
        ax.set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä', fontsize=12)
        ax.set_ylabel('R¬≤', fontsize=12)
        ax.set_ylim([0, 1])
        ax.grid(True, alpha=0.3, axis='y')
        
        for bar, r2 in zip(bars, r2_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{r2:.3f}', ha='center', va='bottom', fontsize=10)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ R2 –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax = axes[0, 1]
        r2_matrix = []
        
        for cluster_id in summary_df['–ö–ª–∞—Å—Ç–µ—Ä']:
            if cluster_id in all_cluster_results:
                cluster_r2 = []
                for col in output_columns:
                    cluster_r2.append(all_cluster_results[cluster_id]['metrics'][f'{col}_R2'])
                r2_matrix.append(cluster_r2)
        
        if r2_matrix:
            r2_matrix = np.array(r2_matrix)
            im = ax.imshow(r2_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')
            
            ax.set_xticks(np.arange(len(output_columns)))
            ax.set_xticklabels(output_columns, rotation=45, ha='right')
            ax.set_yticks(np.arange(len(summary_df)))
            ax.set_yticklabels(summary_df['–ö–ª–∞—Å—Ç–µ—Ä'].astype(str))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏
            for i in range(len(summary_df)):
                for j in range(len(output_columns)):
                    text = ax.text(j, i, f'{r2_matrix[i, j]:.2f}',
                                 ha="center", va="center", color="black", fontsize=8)
            
            ax.set_title('R¬≤ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º', fontsize=14, fontweight='bold')
            plt.colorbar(im, ax=ax, label='R¬≤ score')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax = axes[1, 0]
        cluster_counts = features_df['–ö–ª–∞—Å—Ç–µ—Ä'].value_counts().sort_index()
        
        colors = plt.cm.Set2(np.linspace(0, 1, len(cluster_counts)))
        wedges, texts, autotexts = ax.pie(cluster_counts.values, labels=cluster_counts.index.astype(str),
                                         autopct='%1.1f%%', colors=colors, startangle=90)
        
        ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º', fontsize=14, fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ MSE –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        ax = axes[1, 1]
        mse_values = summary_df['–°—Ä–µ–¥–Ω–∏–π_MSE'].values
        
        ax.bar(clusters, mse_values, color=plt.cm.plasma(np.linspace(0, 1, len(clusters))))
        ax.set_title('–°—Ä–µ–¥–Ω–∏–π MSE –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º', fontsize=14, fontweight='bold')
        ax.set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä', fontsize=12)
        ax.set_ylabel('MSE', fontsize=12)
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_yscale('log')
        
        plt.suptitle('–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø', 
                    fontsize=18, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig('comprehensive_analysis_visualization.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # 2. –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        plt.figure(figsize=(14, 8))
        
        param_avg_r2 = []
        param_avg_mse = []
        
        for col in output_columns:
            r2_values = []
            mse_values = []
            for cluster_id, results in all_cluster_results.items():
                if results:
                    r2_values.append(results['metrics'][f'{col}_R2'])
                    mse_values.append(results['metrics'][f'{col}_MSE'])
            
            if r2_values:
                param_avg_r2.append(np.mean(r2_values))
                param_avg_mse.append(np.mean(mse_values))
        
        x = np.arange(len(output_columns))
        width = 0.35
        
        fig, ax1 = plt.subplots(figsize=(14, 8))
        
        # –ì—Ä–∞—Ñ–∏–∫ R2
        bars1 = ax1.bar(x - width/2, param_avg_r2, width, label='–°—Ä–µ–¥–Ω–∏–π R¬≤', color='skyblue', alpha=0.7)
        ax1.set_xlabel('–ü–∞—Ä–∞–º–µ—Ç—Ä', fontsize=12)
        ax1.set_ylabel('–°—Ä–µ–¥–Ω–∏–π R¬≤', fontsize=12, color='skyblue')
        ax1.set_xticks(x)
        ax1.set_xticklabels(output_columns, rotation=45, ha='right')
        ax1.set_ylim([0, 1])
        ax1.tick_params(axis='y', labelcolor='skyblue')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # –ì—Ä–∞—Ñ–∏–∫ MSE –Ω–∞ –≤—Ç–æ—Ä–æ–π –æ—Å–∏ Y
        ax2 = ax1.twinx()
        bars2 = ax2.bar(x + width/2, param_avg_mse, width, label='–°—Ä–µ–¥–Ω–∏–π MSE', color='salmon', alpha=0.7)
        ax2.set_ylabel('–°—Ä–µ–¥–Ω–∏–π MSE', fontsize=12, color='salmon')
        ax2.tick_params(axis='y', labelcolor='salmon')
        ax2.set_yscale('log')
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≥–µ–Ω–¥—ã
        lines1, labels1 = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        plt.title('–°–†–ê–í–ù–ï–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê –ü–†–û–ì–ù–û–ó–û–í –ü–û –ü–ê–†–ê–ú–ï–¢–†–ê–ú', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('parameter_comparison_analysis.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print("‚úì –°–æ–∑–¥–∞–Ω—ã –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")

def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ LSTM.
    """
    print("=" * 80)
    print("–ú–ù–û–ì–û–ú–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó –° –£–õ–£–ß–®–ï–ù–ù–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò LSTM")
    print("=" * 80)
    print("üéØ –°–¢–†–ê–¢–ï–ì–ò–Ø: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä–∞, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥–æ–º")
    print("üìà –£–õ–£–ß–®–ï–ù–ò–Ø: –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
    print("=" * 80)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    file_path = 'Dataset.xlsx'
    segment_length = 20
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    input_columns = ['kŒ≥', 'kŒ≤', 'Œ±0', 'lœà', 'V0', 'LWx', 'œâ*', 'e1', 'e6', 'F1', 'F6', 'F']
    
    # –í—ã—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ)
    output_columns = ['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6']
    
    try:
        # ===== 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê =====
        print("\n1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê")
        print("-" * 50)
        
        df = pd.read_excel(file_path)
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"‚úì –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {', '.join(input_columns[:5])}...")
        print(f"‚úì –í—ã—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {', '.join(output_columns)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫
        missing_input = [col for col in input_columns if col not in df.columns]
        missing_output = [col for col in output_columns if col not in df.columns]
        
        if missing_input:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_input}")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö
            all_columns = df.columns.tolist()
            input_columns = [col for col in all_columns if col not in output_columns]
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ: {len(input_columns)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        if missing_output:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_output}")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
        nan_counts = df[input_columns + output_columns].isnull().sum()
        total_nan = nan_counts.sum()
        if total_nan > 0:
            print(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ NaN –∑–Ω–∞—á–µ–Ω–∏–π: {total_nan}")
            print("–ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏...")
            df[input_columns + output_columns] = df[input_columns + output_columns].fillna(
                df[input_columns + output_columns].mean()
            )
        
        # ===== 2. –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø –ò –í–´–ß–ò–°–õ–ï–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö =====
        print("\n2. –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø –ò –í–´–ß–ò–°–õ–ï–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö")
        print("-" * 50)
        
        segments = []
        all_features = []
        segment_data_dict = {}
        
        segment_counter = 0
        for i in range(0, len(df), segment_length):
            segment = df.iloc[i:i + segment_length]
            if len(segment) >= 10:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è LSTM
                segment_counter += 1
                seg_id = f"–°–µ–≥–º–µ–Ω—Ç_{segment_counter:03d}"
                segments.append(segment)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                features = calculate_time_series_features(segment, seg_id, i, output_columns)
                if features:
                    all_features.append(features)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–ª—è LSTM
                segment_data_dict[seg_id] = segment[input_columns + output_columns]
        
        if not all_features:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return
            
        features_df = pd.DataFrame(all_features)
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        print(f"‚úì –í—ã—á–∏—Å–ª–µ–Ω–æ {len(all_features)} –Ω–∞–±–æ—Ä–æ–≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º features_df –Ω–∞ NaN
        nan_in_features = features_df.isnull().sum().sum()
        if nan_in_features > 0:
            print(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ NaN –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö: {nan_in_features}")
            print("–ó–∞–ø–æ–ª–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏...")
            features_df = features_df.fillna(features_df.mean())
        
        # ===== 3. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø K-MEANS =====
        print("\n3. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø K-MEANS")
        print("-" * 50)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        features_df, cluster_labels, scaler, cluster_cols, kmeans = perform_kmeans_clustering(
            features_df, n_clusters='auto'
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        features_df.to_excel("improved_clustering_results.xlsx", index=False)
        print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'improved_clustering_results.xlsx'")
        
        # ===== 4. –ö–†–û–°–°-–°–ï–ì–ú–ï–ù–¢–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï =====
        print("\n4. –ö–†–û–°–°-–°–ï–ì–ú–ï–ù–¢–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –° –£–õ–£–ß–®–ï–ù–ù–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò")
        print("-" * 50)
        print("üéØ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:")
        print("   ‚Ä¢ 3-—Å–ª–æ–π–Ω–∞—è LSTM —Å–µ—Ç—å")
        print("   ‚Ä¢ Batch Normalization")
        print("   ‚Ä¢ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è")
        print("   ‚Ä¢ ReduceLROnPlateau callback")
        print("   ‚Ä¢ –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏")
        print("-" * 50)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        cluster_segments = {}
        for seg_id, segment_data in segment_data_dict.items():
            # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            seg_features = features_df[features_df['–°–µ–≥–º–µ–Ω—Ç_ID'] == seg_id]
            if not seg_features.empty:
                cluster_id = seg_features['–ö–ª–∞—Å—Ç–µ—Ä'].iloc[0]
                if cluster_id not in cluster_segments:
                    cluster_segments[cluster_id] = {}
                cluster_segments[cluster_id][seg_id] = segment_data
        
        print(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–ï–ì–ú–ï–ù–¢–û–í –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú:")
        for cluster_id in sorted(cluster_segments.keys()):
            seg_count = len(cluster_segments[cluster_id])
            print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {seg_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        
        # –û–±—É—á–∞–µ–º –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        all_cluster_results = {}
        
        trained_clusters = 0
        for cluster_id, cluster_data in cluster_segments.items():
            if len(cluster_data) >= 2:  # –¢–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –º–∏–Ω–∏–º—É–º 2 —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
                
                predictions, metrics, train_seg_id, test_seg_id = train_and_test_cross_segment(
                    cluster_data, cluster_id, input_columns, output_columns,
                    sequence_length=20, epochs=100
                )
                
                if predictions and metrics:
                    all_cluster_results[cluster_id] = {
                        'predictions': predictions,
                        'metrics': metrics,
                        'train_segment': train_seg_id,
                        'test_segment': test_seg_id
                    }
                    trained_clusters += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                    metrics_df = save_cross_segment_results(
                        cluster_id, predictions, metrics, features_df, output_columns,
                        train_seg_id, test_seg_id
                    )
                else:
                    print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}")
            else:
                print(f"\n‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –ü—Ä–æ–ø—É—â–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 2 —Å–µ–≥–º–µ–Ω—Ç–∞, –¥–æ—Å—Ç—É–ø–Ω–æ: {len(cluster_data)})")
        
        if trained_clusters == 0:
            print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ LSTM")
            return
        
        # ===== 5. –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====
        print("\n5. –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("-" * 50)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        summary_df = create_comprehensive_analysis(all_cluster_results, output_columns, features_df)
        
        if summary_df is not None:
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"   –û–±—É—á–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {trained_clusters}")
            print(f"   –°—Ä–µ–¥–Ω–∏–π R¬≤ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].mean():.4f}")
            print(f"   –°—Ä–µ–¥–Ω–∏–π MSE –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {summary_df['–°—Ä–µ–¥–Ω–∏–π_MSE'].mean():.8f}")
            print(f"   –õ—É—á—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä (R¬≤={summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].max():.4f}): –ö–ª–∞—Å—Ç–µ—Ä {summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].idxmax()}")
            print(f"   –•—É–¥—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä (R¬≤={summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].min():.4f}): –ö–ª–∞—Å—Ç–µ—Ä {summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].idxmin()}")
        
        # ===== 6. –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–¢–û–ì–û–í–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====
        print("\n6. –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–¢–û–ì–û–í–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("-" * 50)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
        all_metrics_list = []
        for cluster_id, results in all_cluster_results.items():
            all_metrics_list.append(results['metrics'])
        
        if all_metrics_list:
            all_metrics_df = pd.DataFrame(all_metrics_list)
            all_metrics_df.to_excel("all_improved_metrics.xlsx", index=False)
            print("‚úì –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'all_improved_metrics.xlsx'")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        models_info = {
            '–í—Å–µ–≥–æ_–∫–ª–∞—Å—Ç–µ—Ä–æ–≤': len(cluster_segments),
            '–û–±—É—á–µ–Ω–æ_–∫–ª–∞—Å—Ç–µ—Ä–æ–≤': trained_clusters,
            '–°—Ç—Ä–∞—Ç–µ–≥–∏—è_–æ–±—É—á–µ–Ω–∏—è': '–ö—Ä–æ—Å—Å-—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏',
            '–í—Ö–æ–¥–Ω—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã': input_columns,
            '–í—ã—Ö–æ–¥–Ω—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã': output_columns,
            '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞_LSTM': {
                'layers': 'LSTM(64)-BN-Dropout-LSTM(32)-BN-Dropout-LSTM(16)-BN-Dropout-Dense(64)-BN-Dropout-Dense(32)-BN-Dense(7)',
                'regularization': 'L2 regularization',
                'optimizer': 'Adam with learning rate scheduling',
                'callbacks': 'EarlyStopping, ReduceLROnPlateau'
            },
            '–ü–∞—Ä–∞–º–µ—Ç—Ä—ã_–æ–±—É—á–µ–Ω–∏—è': {
                'sequence_length': 10,
                'epochs': 100,
                'batch_size': 32,
                'validation_split': 0.2
            }
        }
        
        import json
        with open('improved_analysis_config.json', 'w', encoding='utf-8') as f:
            json.dump(models_info, f, indent=2, ensure_ascii=False)
        
        print("‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'improved_analysis_config.json'")
        
        # ===== 7. –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ =====
        print("\n7. –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        print("-" * 50)
        
        print("\n" + "=" * 80)
        print("–£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 80)
        
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   üìä –û–±—É—á–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {trained_clusters}")
        print(f"   üìà –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å (R¬≤): {summary_df['–°—Ä–µ–¥–Ω–∏–π_R2'].mean():.4f}" if summary_df is not None else "")
        print(f"   üìâ –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ (MSE): {summary_df['–°—Ä–µ–¥–Ω–∏–π_MSE'].mean():.8f}" if summary_df is not None else "")
        
        print(f"\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        print(f"   1. improved_clustering_results.xlsx - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        print(f"   2. all_improved_metrics.xlsx - –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π")
        print(f"   3. comprehensive_analysis_summary.xlsx - —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞")
        print(f"   4. improved_analysis_config.json - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
        print(f"   5. comprehensive_analysis_visualization.png - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
        print(f"   6. parameter_comparison_analysis.png - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        print(f"   7. –ü–∞–ø–∫–∏ cluster_X_results/ - –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
        
        print(f"\nüìÇ –í –ü–ê–ü–ö–ê–• CLUSTER_X_RESULTS/ –°–û–î–ï–†–ñ–ê–¢–°–Ø:")
        print(f"   ‚Ä¢ –î–µ—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞")
        print(f"   ‚Ä¢ –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—à–∏–±–æ–∫")
        print(f"   ‚Ä¢ –¢–∞–±–ª–∏—Ü—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏")
        print(f"   ‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        print(f"\nüìà –£–õ–£–ß–®–ï–ù–ò–Ø –¢–û–ß–ù–û–°–¢–ò:")
        print(f"   ‚Ä¢ –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM (3 —Å–ª–æ—è)")
        print(f"   ‚Ä¢ Batch Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è")
        print(f"   ‚Ä¢ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
        print(f"   ‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate (ReduceLROnPlateau)")
        print(f"   ‚Ä¢ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è")
        
    except FileNotFoundError:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ TensorFlow
    try:
        import tensorflow as tf
        print(f"TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}")
        gpu_devices = tf.config.list_physical_devices('GPU')
        gpu_available = len(gpu_devices) > 0
        print(f"GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpu_available}")
        if gpu_available:
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {gpu_devices[0]}")
    except ImportError:
        print("‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensorflow")
        exit(1)
    
    main()